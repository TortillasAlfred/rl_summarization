#####################
### MISCELLANEOUS ###
#####################
seed: 42 # Random seed
dev: False
slurm_log_path: /project/def-lulam50/magod/rl_summ/slurm_outputs/

##################
### MODEL ARGS ###
##################
model: "linsit_pretraining" # One of ['lead3', 'banditsum', 'banditsum_mcts', 'rlsum_mcts', 'azsum_mcts', 'a2c', 'rlsum_oh']
train_batch_size: 32
test_batch_size: 256
hidden_dim: 200
decoder_dim: 100
n_repeats_per_sample: 16
learning_rate: 0.001
epsilon: 1.0
epsilon_min: 0.05
epsilon_decay: 0.99
n_sents_per_summary: 3
n_jobs_for_mcts: 4
lambda_loss: 0.5
warmup_batches: 10000
pretraining_path: "/project/def-lulam50/magod/rl_summ/exp_logging/pretraining/"

######################
### MCTS OFUL ARGS ###
######################
lambda_oful: 1.0
alpha_oful: 1.0
S: 1.0
R: 1.0
delta: 0.0001
D_t_source: "word-level"

####################
### DATASET ARGS ###
####################
dataset: "cnn_dailymail" # One of ['cnn_dailymail']
data_path: "/scratch/magod/summarization_datasets/cnn_dailymail/data/" # Argument to the Dataset object
embeddings: "glove.6B.100d"
embeddings_location: "/scratch/magod/embeddings/"
sets: "train-val-test" # If present, 'train' must be first

###################
### REWARD ARGS ###
###################
reward: "rouge" # One of ['rouge']

##################
### MCTS ARGS  ###
##################
c_puct: 1.0
n_mcts_samples: 2500
dirichlet_epsilon: 0.25

####################
### TRAINER ARGS ###
####################
trainer: "pl" # One of ['gradient_free', 'pl']
gradient_clip_val: 1
gpus: -1
overfit_pct: 0.0
fast_dev_run: False
distributed_backend: "dp" # one of ['dp', 'ddp', 'ddp2']
val_check_interval: 1.0
default_save_path: "/project/def-lulam50/magod/rl_summ/exp_logging/results/"
weights_save_path: "/project/def-lulam50/magod/rl_summ/exp_logging/weight_saving"
max_epochs: 4
weight_decay: 0.000001
dropout: 0.0
accumulate_grad_batches: 1
