#####################
### MISCELLANEOUS ###
#####################
seed: 42 # Random seed
dev: False
slurm_log_path: "/project/def-adurand/magod/rl_summ/slurm_outputs/"

##################
### MODEL ARGS ###
##################
model: "bertcombisum" # One of ['bertcombisum', 'banditsum', 'banditsum_mcts', 'rlsum_mcts', 'azsum_mcts', 'a2c', 'rlsum_oh']
encoder: "MLPClassifier" # One of ['Transformer', 'MLPClassifier']
MLPClassifier_size: "small" # One of ["small", "medium", "large"]
num_inter_sentence_transformer: 2
num_head: 4
num_workers: 0
train_batch_size: 12
test_batch_size: 24
hidden_dim: 200
decoder_dim: 100
n_repeats_per_sample: 16
learning_rate: 0.00005
epsilon: 0.1
n_sents_per_summary: 3
n_jobs_for_mcts: 4

####################
### DATASET ARGS ###
####################
dataset: "cnn_dailymail_bert" # One of ['cnn_dailymail, cnn_dailymail_bert']
data_path: "$SLURM_TMPDIR/data" # Argument to the Dataset object
embeddings: "glove.6B.100d"
embeddings_location: "/project/def-adurand/magod/embeddings/"
sets: "train-val-test" # If present, 'train' must be first
bert_cache: "" # If no internet connection, save pretrained parameters
  # for BertTokenizerFast and BertModel in bert_cache path
load_data_tokenized: False #Pickle/Load the tokenized data
store_data_tokenized: False #Pickle/Store the tokenized data
max_sents_per_doc: 50
max_len_sent: 80
min_len_sent: 0
max_tokens_per_doc: 512

###################
### REWARD ARGS ###
###################
reward: "rouge" # One of ['rouge']

#################
### UCB ARGS  ###
#################
c_puct: 10.0
ucb_sampling: "linear" # One of ["fix", "linear"]

####################
### TRAINER ARGS ###
####################
trainer: "pl" # One of ['gradient_free', 'pl']
gradient_clip_val: 1
gpus: 1
overfit_pct: 0.0
fast_dev_run: False
distributed_backend: "dp" # one of ['dp', 'ddp', 'ddp2']
val_check_interval: 0.1
default_save_path: "/project/def-adurand/jegauth/rl_summ/results"
weights_save_path: "/project/def-adurand/jegauth/rl_summ/weights"
max_epochs: 20
weight_decay: 0.000001
dropout: 0.0
accumulate_grad_batches: 1

########################
### JOB INDEX sur CC ###
########################
job_index: -1