import time

from os import listdir
from os.path import isfile, join
from torch.utils.data import Dataset
from datasets import load_dataset
from transformers import BertTokenizerFast


class CnnDailyMailDatasetBert():
    def __init__(self, data_path):
        self.loaded_data = self._load_dataset(join(data_path, 'finished_files' , 'train'), 
                                              join(data_path, 'finished_files' ,'test'),
                                              join(data_path, 'finished_files/val/'))
        self.dataset = self.tokenized_dataset(self.loaded_data)

    def _load_dataset(self, train_dir, test_dir, val_dir, cache_dir="./cache_dir"): 
        """Utility method use to load the data

        Args:
            train_dir (str): path to train directory containing files (.json) for training
            test_dir (str): path to test directory containing files (.json) for testing
            val_dir (str): path to train directory containing files (.json) for validation
            cache_dir (str): path to cache directory used by `datasets.load_dataset` 
        
        Returns:
            DatasetDict: return the dataset loaded from train_dir, test_dir and val_dir
        """

        train_files = [join(train_dir, f) for f in listdir(train_dir) if isfile(join(train_dir, f))]
        test_files = [join(test_dir, f) for f in listdir(test_dir) if isfile(join(test_dir, f))]
        val_files = [join(val_dir, f) for f in listdir(val_dir) if isfile(join(val_dir, f))]

        return load_dataset('json', 
                            data_files={"train": train_files, "test": test_files, "val": val_files},
                            cache_dir=cache_dir)
        
    def encode(self, document, tokenizer): 
        """Utility method used to preprocess and tokenize the data
        
        Args:
            document (list): list of lists of sentences to preprocess and tokenize
            tokenizer (BertTokenizerFast): object method used to preprocess and tokenize 

        Return:
            dict: dictionary with keys `input_ids`, `token_type_ids` and `attention_mask`
        """
        return tokenizer(document, 
                        max_length=80,
                        add_special_tokens=True,
                        padding=True,
                        truncation=True,
                        return_tensors='pt')

    def tokenized_dataset(self, dataset):
        """ Method that tokenizes each document in the train, test and validation dataset

        Args:
            dataset (DatasetDict): dataset that will be tokenized (train, test, validation)
        
        Returns:
            dict: dataset once tokenized
        """
        tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

        print("\n"+"="*10, "Start Tokenizing", "="*10)
        start = time.process_time()
        train_articles = [self.encode(document, tokenizer) for document in dataset["train"]["article"]]
        test_articles = [self.encode(document, tokenizer) for document in dataset["test"]["article"]]
        val_articles = [self.encode(document, tokenizer) for document in dataset["val"]["article"]]
        train_abstracts = [self.encode(document, tokenizer) for document in dataset["train"]["abstract"]]
        test_abstracts = [self.encode(document, tokenizer) for document in dataset["test"]["abstract"]]
        val_abstracts = [self.encode(document, tokenizer) for document in dataset["val"]["abstract"]]
        print("Time:", time.process_time() - start)
        print("=" * 10, "End Tokenizing", "="*10+"\n")

        return {"train": (dataset["train"]["id"], train_articles, train_abstracts),
                "test": (dataset["train"]["id"], test_articles, test_abstracts),
                "val": (dataset["val"]["id"], val_articles, val_abstracts)}


class Example():
    """Utility data type used to create objects"""

    def __init__(self, id=None, content=None, abstract=None):
        self.id = id
        self.content = content
        self.abstract = abstract


class DatasetBertWrapper(CnnDailyMailDatasetBert):
    """ Wrapper used to reformat the data generated by CnnDailyMailDatasetBert """

    def __init__(self, data_path):
        super().__init__(data_path)
        self.fields = {"id": Example(), "content": Example(), "abstract": Example()}
        self.subsets = self.reformat_data()
        self.pad_idx = 1 

    def reformat_data(self):
        """Method used to reformat the data genrated by CnnDailMailDatasetBert"""
        return {name: [Example(*args) for args in zip(documents[0], documents[1], documents[2])] 
                for name, documents in self.dataset.items()}

    def get_splits(self):
        """Method used to build the dataloader for train, test and validation"""
        return {name: TextDatasetBert(dataset, self.fields) for name, dataset in self.subsets.items()}

    @staticmethod
    def from_config(config):
        """Method used to create object on the fly (Factory Design Pattern)
        
        Args:
            config: configuration to fill different parameters and hyperparameters 
        """
        return DatasetBertWrapper(config.data_path)


class TextDatasetBert(Dataset):
    def __init__(self, dataset, fields):
        self.examples = [
            self.__process_example(x, fields) for x in dataset
        ]

    def __process_example(self, x, fields):
        return {name: getattr(x, name) for name, f in fields.items()}

    def subset(self, n):
        self.examples = self.examples[:n]

    def __getitem__(self, i):
        return self.examples[i]

    def __len__(self):
        return len(self.examples)

    def __iter__(self):
        for x in self.examples:
            yield x
